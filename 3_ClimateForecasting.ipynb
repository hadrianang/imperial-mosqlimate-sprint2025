{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b723fd72-8855-4b32-9b11-930bf44be096",
   "metadata": {},
   "source": [
    "# 3_ClimateForecasting\n",
    "In this notebook, we read in the results of the climate model tuning process, train a model based on this, evaluate it on the test set, then generate the multiple sets of climate forecasts needed for the dengue model tuning and training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75f0eb-557e-468e-a28d-56ecdafe11c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import os\n",
    "import json\n",
    "\n",
    "from darts import TimeSeries, concatenate\n",
    "from darts.utils.callbacks import TFMProgressBar\n",
    "from darts.metrics import mape, smape, mse, rmse, mae, mql, ql\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.dataprocessing.transformers import StaticCovariatesTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from darts.models import (\n",
    "    RandomForestModel,\n",
    "    LinearRegressionModel,\n",
    "    XGBModel\n",
    ")\n",
    "from darts.utils.likelihood_models import GaussianLikelihood, PoissonLikelihood, QuantileRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning.callbacks import Callback, EarlyStopping\n",
    "import pickle\n",
    "import helpers as hp\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "random_seed = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45f8f45-e545-47a5-aa25-acba690c2c1d",
   "metadata": {},
   "source": [
    "# 1. Reading Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c688dc12-dcac-4565-961b-e1987028bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_name = \"Notebook_24_07_2025\"\n",
    "base_dir = os.getcwd()\n",
    "model_input_dir = os.path.join(base_dir, \"ModelInput\")\n",
    "tuning_dir = os.path.join(base_dir, \"Climate Tuning\", tuning_name)\n",
    "\n",
    "weekly_cal = pd.read_csv(os.path.join(model_input_dir, \"Calendar.csv\"))\n",
    "climate_df = pd.read_csv(os.path.join(model_input_dir, \"TimeVaryingCovs.csv\"))\n",
    "\n",
    "#Remove uf = \"ES\" since there is no dengue data for Espirito Santo\n",
    "climate_df = climate_df[climate_df[\"uf\"] != \"ES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611540fa-3bc5-49a8-8da6-511d0e3854d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_read = \"RandomForest\"\n",
    "tuning_res_dir = os.path.join(tuning_dir, model_to_read)\n",
    "forecast_res_dir = os.path.join(tuning_res_dir, \"Forecasts\")\n",
    "os.makedirs(forecast_res_dir, exist_ok = True)\n",
    "model_best_params = hp.read_json(os.path.join(tuning_res_dir, \"best_params.json\"))\n",
    "tuning_config = hp.read_json(os.path.join(tuning_dir, \"tuning_details.json\"))\n",
    "\n",
    "clim_var_names = tuning_config[\"clim_var_names\"]\n",
    "global_scale = tuning_config[\"global_scale\"]\n",
    "retrain_per_fold = tuning_config[\"retrain_per_fold\"] \n",
    "approach = tuning_config[\"approach\"]\n",
    "use_orig_scaler = False #If we use the original scaler during tuning to rescale values\n",
    "use_orig_normer = True #Use the original train1 series (same as tuning) for the normalisation of error metrics - makes test loss comparable with validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75769db-a907-4b4e-994a-18e6ff7763c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define set info used to split the data into different sets. This section should ideally align with what was used during hype\n",
    "set_info = weekly_cal[[\"epiweek\", \"Year\", \"Week\", \"WeekStart\", \"WeekMid\", \"WeekEnd\"]]\n",
    "set_info = set_info.iloc[1:]\n",
    "\n",
    "#Define the data sets using a column of booleans in the calendar. \n",
    "#all epiweek numbers are in YYYYWW integer format\n",
    "set_info[\"clim_train1\"] = set_info[\"epiweek\"] <= 201510\n",
    "set_info[\"clim_val1\"] = (set_info[\"epiweek\"] >= 201526) & (set_info[\"epiweek\"] <= 201625)\n",
    "set_info[\"clim_train2\"] = set_info[\"epiweek\"] <= 201610\n",
    "set_info[\"clim_val2\"] = (set_info[\"epiweek\"] >= 201626) & (set_info[\"epiweek\"] <= 201725)\n",
    "set_info[\"clim_train3\"] = set_info[\"epiweek\"] <= 201710\n",
    "set_info[\"clim_val3\"] = (set_info[\"epiweek\"] >= 201726) & (set_info[\"epiweek\"] <= 201825)\n",
    "set_info[\"clim_test\"] = (set_info[\"epiweek\"] >= 201841) & (set_info[\"epiweek\"] <= 201940)\n",
    "set_info[\"clim_test_input\"] = set_info[\"epiweek\"] <= 201825\n",
    "set_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bb8adc-7f8b-4ad0-ba51-cb6200c27315",
   "metadata": {},
   "outputs": [],
   "source": [
    "uf_list = climate_df[\"uf\"].unique()\n",
    "uf_mapper = climate_df[[\"uf\", \"uf_name\"]].copy().drop_duplicates()\n",
    "uf_dict = dict(zip(uf_mapper[\"uf\"], uf_mapper[\"uf_name\"]))\n",
    "uf_name_list = [uf_dict[curr_uf] for curr_uf in uf_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b3580-dd96-49e1-af19-6771e9c64266",
   "metadata": {},
   "source": [
    "# 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e02e2a6-e931-46d1-8a8c-a8f7af29fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a version of the climate DataFrame where the precipiation related variables have been log transformed\n",
    "log_trans_cols = [\"precip_min\", \"precip_med\", \"precip_max\"]\n",
    "proc_climate_df = hp.transform_df_cols(climate_df, log_trans_cols, np.log1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c76e6b-1723-4007-81fe-a182402c378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_col = \"uf\"\n",
    "\n",
    "#Training set without the pre-processing, mainly for normalising error metrics\n",
    "train1 = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_train1\")\n",
    "train2 = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_train2\")\n",
    "train3 = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_train3\")\n",
    "\n",
    "#Training sets with pre-processing for actual model training\n",
    "train1_log = hp.create_ts_list(proc_climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_train1\")\n",
    "train2_log = hp.create_ts_list(proc_climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_train2\")\n",
    "train3_log = hp.create_ts_list(proc_climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_train3\")\n",
    "\n",
    "#Note that since the validation and test sets are only used for evaluation / error calculation, we use the non pre-processed version (no log-transforms) \n",
    "val1 = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_val1\")\n",
    "val2 = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_val2\")\n",
    "val3 = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_val3\")\n",
    "\n",
    "eval_train = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_test_input\")\n",
    "eval_train_log = hp.create_ts_list(proc_climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_test_input\") # Input sequence for the final model to evaluate on test set\n",
    "test = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_test\") #Test set for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b206da-0d06-4a91-ba78-c57cac6eab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original scaler\n",
    "clim_scaler1 = Scaler(global_fit = global_scale)\n",
    "clim_scaler1.fit(train1_log)\n",
    "\n",
    "#Only scale training sets \n",
    "train1_log_s = clim_scaler1.transform(train1_log)\n",
    "train2_log_s = clim_scaler1.transform(train2_log)\n",
    "train3_log_s = clim_scaler1.transform(train3_log)\n",
    "\n",
    "#New Scaler\n",
    "clim_eval_scaler = Scaler(global_fit = global_scale)\n",
    "if use_orig_scaler:\n",
    "    eval_train_log_s = clim_scaler1.transform(eval_train_log)\n",
    "else:\n",
    "    eval_train_log_s = clim_eval_scaler.fit_transform(eval_train_log)\n",
    "\n",
    "if use_orig_normer:\n",
    "    normer = train1\n",
    "else:\n",
    "    normer = eval_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408d3b5-cb09-4be4-a4f4-6a84ce11037f",
   "metadata": {},
   "source": [
    "# 3. Fitting Model\n",
    "We then use the best hyperparameters from the tuning process to fit a model (same type as was tuned), evaluate it against the test set, and then generate the necessary forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a517ad-2a03-4dfa-880c-4dd788230e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell should match the definitions listed in the tuning process\n",
    "gap_length = 15\n",
    "\n",
    "#We use 53 as the base forecast length because some years have 53 weeks\n",
    "if approach == \"forecast_gap\": #\"Forecast the gap\" approach\n",
    "    output_chunk_length = 1 #We assume 1-step ahead for the forecast the gap approach\n",
    "    output_chunk_shift = 0\n",
    "    forecast_length = 53 + gap_length\n",
    "else: #\"One shot forecast\" approach\n",
    "    output_chunk_length = 53 #If we are doing a 1-shot forecast, we set output_chunk_length to 53 since some years have 53 weeks.\n",
    "    output_chunk_shift = gap_length #We set an output_chunk_shift equal to the gap size\n",
    "    forecast_length = output_chunk_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30417b6-9bc8-4d49-9285-f993e347c67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = model_best_params[\"lags\"]\n",
    "encoders = {\"datetime_attribute\": {\"future\": [\"month\", \"year\"]}, \n",
    "                \"transformer\": Scaler()}\n",
    "\n",
    "if model_to_read == \"LinearRegression\":\n",
    "    main_model = LinearRegressionModel(lags = lags, lags_future_covariates = (lags, output_chunk_length), \n",
    "                                       add_encoders = encoders, output_chunk_length = output_chunk_length, \n",
    "                                       output_chunk_shift = output_chunk_shift)\n",
    "elif model_to_read == \"RandomForest\":\n",
    "    main_model = RandomForestModel(lags = lags, lags_future_covariates = (lags, output_chunk_length), add_encoders = encoders, \n",
    "                                   output_chunk_length = output_chunk_length, output_chunk_shift = output_chunk_shift, \n",
    "                                   random_state = random_seed)\n",
    "else:\n",
    "    main_model = XGBModel(lags = lags, lags_future_covariates = (lags, output_chunk_length), add_encoders = encoders, \n",
    "                                   output_chunk_length = output_chunk_length, output_chunk_shift = output_chunk_shift, \n",
    "                                   random_state = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0da6ea6-f88d-47d6-8b88-31c5045e6d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model.fit(eval_train_log_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f19e9f-18a2-44cc-9613-425c74c97ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_preds_log_s = main_model.predict(series = eval_train_log_s, n = forecast_length)\n",
    "if use_orig_scaler:\n",
    "    eval_preds_log = clim_scaler1.inverse_transform(eval_preds_log_s)\n",
    "else:\n",
    "    eval_preds_log = clim_eval_scaler.inverse_transform(eval_preds_log_s)\n",
    "\n",
    "eval_preds = hp.transform_comps_ts_list(eval_preds_log, comps_to_trans = log_trans_cols, trans_func = np.expm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac364d-2a29-4a27-91a7-8d65ec3c2131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_metrics_summ = hp.generate_series_metrics_table(test, eval_preds, uf_name_list, \"uf_name\", normer_list = normer, disp_mode = False)\n",
    "test_metrics_per_comp = hp.generate_component_metric_table(test, eval_preds, uf_name_list, \"uf_name\", clim_var_names, mae, normer_list = normer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0cee5a-d1b8-40c0-82dc-916f24493e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.generate_preds_obs_plot(eval_train, test, eval_preds, uf_list, [\"PE\", \"AM\", \"RJ\", \"SP\", \"RR\"], clim_var_names, label_list = uf_name_list, train_limit = 104)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b92569-316c-4986-82b1-e7c98727bfe6",
   "metadata": {},
   "source": [
    "# 4. Generating Forecasts\n",
    "After fitting and evaluating the model, we then generate forecasts. Note that we apply the model that we have evaluated here (we do not retrain it as we generate these forecasts, only changing the input sequence). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b615f37-e382-4181-a43e-c80957bddcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_set_info = weekly_cal[[\"epiweek\", \"Year\", \"Week\", \"WeekStart\", \"WeekMid\", \"WeekEnd\"]]\n",
    "input_set_info = input_set_info.iloc[1:]\n",
    "\n",
    "#Sets for input series to generate forecasts to be used during hyperparameter tuning\n",
    "input_set_info[\"clim_input_1a\"] = input_set_info[\"epiweek\"] <= 201825\n",
    "input_set_info[\"clim_input_1b\"] = input_set_info[\"epiweek\"] <= 201925\n",
    "input_set_info[\"clim_input_1c\"] = input_set_info[\"epiweek\"] <= 202025\n",
    "\n",
    "#Validation sets, which we can also use to see if our forecasts have similar loss to the test and validation sets\n",
    "input_set_info[\"clim_val_1a\"] = (input_set_info[\"epiweek\"] >= 201841) & (input_set_info[\"epiweek\"] <= 201940)\n",
    "input_set_info[\"clim_val_1b\"] = (input_set_info[\"epiweek\"] >= 201941) & (input_set_info[\"epiweek\"] <= 202040)\n",
    "input_set_info[\"clim_val_1c\"] = (input_set_info[\"epiweek\"] >= 202041) & (input_set_info[\"epiweek\"] <= 202140)\n",
    "\n",
    "\n",
    "#Input sets for generating forecasts for early stopping and validation loss during dengue model training\n",
    "input_set_info[\"clim_input_1\"] = input_set_info[\"epiweek\"] <= 202110\n",
    "input_set_info[\"clim_input_2\"] = input_set_info[\"epiweek\"] <= 202210\n",
    "input_set_info[\"clim_input_3\"] = input_set_info[\"epiweek\"] <= 202310\n",
    "\n",
    "input_set_info[\"clim_val_1\"] = (input_set_info[\"epiweek\"] >= 202126) & (input_set_info[\"epiweek\"] <= 202225)\n",
    "input_set_info[\"clim_val_2\"] = (input_set_info[\"epiweek\"] >= 202226) & (input_set_info[\"epiweek\"] <= 202325)\n",
    "input_set_info[\"clim_val_3\"] = (input_set_info[\"epiweek\"] >= 202326) & (input_set_info[\"epiweek\"] <= 202425)\n",
    "\n",
    "#Hold sets - which are also used as inputs to generate forecasts over the test sets\n",
    "input_set_info[\"clim_hold_1\"] = input_set_info[\"epiweek\"] <= 202225\n",
    "input_set_info[\"clim_hold_2\"] = input_set_info[\"epiweek\"] <= 202325\n",
    "input_set_info[\"clim_hold_3\"] = input_set_info[\"epiweek\"] <= 202425\n",
    "\n",
    "#Targets - to evaluate the model output when hold sets are inputs. Forecasts over targets used in evaluating the model\n",
    "input_set_info[\"clim_target_1\"] = (input_set_info[\"epiweek\"] >= 202241) & (input_set_info[\"epiweek\"] <= 202340)\n",
    "input_set_info[\"clim_target_2\"] = (input_set_info[\"epiweek\"] >= 202341) & (input_set_info[\"epiweek\"] <= 202440)\n",
    "input_set_info[\"clim_target_3\"] = (input_set_info[\"epiweek\"] >= 202441) & (input_set_info[\"epiweek\"] <= 202540)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920278d6-e1b2-400f-980e-6c66116629bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create all the TimeSeries (both raw and log-transformed versions)\n",
    "\n",
    "#For tuning\n",
    "#Input 1a, b, c to generate forecasts for the validation sets in hyperparameter tuning\n",
    "clim_input_1a = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_input_1a\")\n",
    "clim_input_1b = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_input_1b\")\n",
    "clim_input_1c = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_input_1c\")\n",
    "\n",
    "#To compare with forecasts using input 1a,b,c\n",
    "clim_val_1a = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_val_1a\")\n",
    "clim_val_1b = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_val_1b\")\n",
    "clim_val_1c = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_val_1c\")\n",
    "\n",
    "#For training\n",
    "#Input 1, 2, 3 to generate forecasts for the validation sets used in early stopping of the main model training process\n",
    "clim_input_1 = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_input_1\")\n",
    "clim_input_2 = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_input_2\")\n",
    "clim_input_3 = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_input_3\")\n",
    "\n",
    "#Climate during the dengue validation sets (can evaluate against output from clim_input 1, 2, 3)\n",
    "clim_val_1 = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_val_1\")\n",
    "clim_val_2 = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_val_2\")\n",
    "clim_val_3 = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_val_3\")\n",
    "\n",
    "#For evaluation\n",
    "#Hold 1, 2, 3 will also be used as input to generate \n",
    "clim_hold_1 = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_hold_1\")\n",
    "clim_hold_2 = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_hold_2\")\n",
    "clim_hold_3 = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_hold_3\")\n",
    "\n",
    "clim_target_1 = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_target_1\")\n",
    "clim_target_2 = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_target_2\")\n",
    "clim_target_3 = hp.create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_target_3\")\n",
    "\n",
    "#Log transformed versions of the TimeSeries that will be used as model input\n",
    "clim_input_1a_log = hp.create_ts_list(proc_climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_input_1a\")\n",
    "clim_input_1b_log = hp.create_ts_list(proc_climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_input_1b\")\n",
    "clim_input_1c_log = hp.create_ts_list(proc_climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_input_1c\")\n",
    "\n",
    "clim_input_1_log = hp.create_ts_list(proc_climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_input_1\")\n",
    "clim_input_2_log = hp.create_ts_list(proc_climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_input_2\")\n",
    "clim_input_3_log = hp.create_ts_list(proc_climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_input_3\")\n",
    "\n",
    "clim_hold_1_log = hp.create_ts_list(proc_climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_hold_1\")\n",
    "clim_hold_2_log = hp.create_ts_list(proc_climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_hold_2\")\n",
    "clim_hold_3_log = hp.create_ts_list(proc_climate_df, clim_var_names, group_col, uf_list, set_info = input_set_info, set_column = \"clim_hold_3\")\n",
    "\n",
    "#Scale the values\n",
    "if use_orig_scaler:\n",
    "    scaler_to_use = clim_scaler1\n",
    "else:\n",
    "    scaler_to_use = clim_eval_scaler\n",
    "    \n",
    "clim_input_1a_log_s = scaler_to_use.transform(clim_input_1a_log)\n",
    "clim_input_1b_log_s = scaler_to_use.transform(clim_input_1b_log)\n",
    "clim_input_1c_log_s = scaler_to_use.transform(clim_input_1c_log)\n",
    "\n",
    "clim_input_1_log_s = scaler_to_use.transform(clim_input_1_log)\n",
    "clim_input_2_log_s = scaler_to_use.transform(clim_input_2_log)\n",
    "clim_input_3_log_s = scaler_to_use.transform(clim_input_3_log)\n",
    "\n",
    "clim_hold_1_log_s = scaler_to_use.transform(clim_hold_1_log)\n",
    "clim_hold_2_log_s = scaler_to_use.transform(clim_hold_2_log)\n",
    "clim_hold_3_log_s = scaler_to_use.transform(clim_hold_3_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2233f8-2666-4a69-a93d-05881586dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the model to create predictions\n",
    "\n",
    "#First set of predictions - to compare with val1a,b,c - used for tuning dengue model hyperparameters\n",
    "clim_preds_1a_log_s = main_model.predict(series = clim_input_1a_log_s, n = forecast_length)\n",
    "clim_preds_1b_log_s = main_model.predict(series = clim_input_1b_log_s, n = forecast_length)\n",
    "clim_preds_1c_log_s = main_model.predict(series = clim_input_1c_log_s, n = forecast_length)\n",
    "\n",
    "#Second set of predictions to compare with val1,2,3 - used as covariates during early stopping for main predictions\n",
    "clim_preds_1_log_s = main_model.predict(series = clim_input_1_log_s, n = forecast_length)\n",
    "clim_preds_2_log_s = main_model.predict(series = clim_input_2_log_s, n = forecast_length)\n",
    "clim_preds_3_log_s = main_model.predict(series = clim_input_3_log_s, n = forecast_length)\n",
    "\n",
    "#Third set of predictions to compare to target 1,2,3 used as covariates during prediction over the test set \n",
    "clim_preds_targ1_log_s = main_model.predict(series = clim_hold_1_log_s, n = forecast_length)\n",
    "clim_preds_targ2_log_s = main_model.predict(series = clim_hold_2_log_s, n = forecast_length)\n",
    "clim_preds_targ3_log_s = main_model.predict(series = clim_hold_3_log_s, n = forecast_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0822fc8f-cf37-4169-bcba-c8538e9f014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the predictions to natural scale for error metrics computation and visualisation\n",
    "#First set - those for val1 a,b,c for hyperparameter tuning\n",
    "clim_preds_1a = hp.return_to_nat_scale(clim_preds_1a_log_s, log_trans_cols, scaler_to_use)\n",
    "clim_preds_1b = hp.return_to_nat_scale(clim_preds_1b_log_s, log_trans_cols, scaler_to_use)\n",
    "clim_preds_1c = hp.return_to_nat_scale(clim_preds_1c_log_s, log_trans_cols, scaler_to_use)\n",
    "\n",
    "#Second set - those for val1,2,3, which are for early stopping for training dengue models\n",
    "clim_preds_1 = hp.return_to_nat_scale(clim_preds_1_log_s, log_trans_cols, scaler_to_use)\n",
    "clim_preds_2 = hp.return_to_nat_scale(clim_preds_2_log_s, log_trans_cols, scaler_to_use)\n",
    "clim_preds_3 = hp.return_to_nat_scale(clim_preds_3_log_s, log_trans_cols, scaler_to_use)\n",
    "\n",
    "#Third set - those across target1,2,3 for evaluating the trained dengue models\n",
    "clim_preds_targ1 = hp.return_to_nat_scale(clim_preds_targ1_log_s, log_trans_cols, scaler_to_use)\n",
    "clim_preds_targ2 = hp.return_to_nat_scale(clim_preds_targ2_log_s, log_trans_cols, scaler_to_use)\n",
    "clim_preds_targ3 = hp.return_to_nat_scale(clim_preds_targ3_log_s, log_trans_cols, scaler_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a40806-5334-4c24-a36f-9cc7eb64af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invert just the scaling, which should allow us to use these values (with the log transforms) straight away. \n",
    "#First set - those for val1 a,b,c for hyperparameter tuning\n",
    "clim_preds_1a_log = scaler_to_use.inverse_transform(clim_preds_1a_log_s)\n",
    "clim_preds_1b_log = scaler_to_use.inverse_transform(clim_preds_1b_log_s)\n",
    "clim_preds_1c_log = scaler_to_use.inverse_transform(clim_preds_1c_log_s)\n",
    "\n",
    "#Second set - those for val1,2,3, which are for early stopping for training dengue models\n",
    "clim_preds_1_log = scaler_to_use.inverse_transform(clim_preds_1_log_s)\n",
    "clim_preds_2_log = scaler_to_use.inverse_transform(clim_preds_2_log_s)\n",
    "clim_preds_3_log = scaler_to_use.inverse_transform(clim_preds_3_log_s)\n",
    "\n",
    "#Third set - those across target1,2,3 for evaluating the trained dengue models\n",
    "clim_preds_targ1_log = scaler_to_use.inverse_transform(clim_preds_targ1_log_s)\n",
    "clim_preds_targ2_log = scaler_to_use.inverse_transform(clim_preds_targ2_log_s)\n",
    "clim_preds_targ3_log = scaler_to_use.inverse_transform(clim_preds_targ3_log_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245cd759-0a9c-4f67-bb77-fc57b8d31a46",
   "metadata": {},
   "source": [
    "# 5. Evaluating Forecasts\n",
    "We compute evaluation metrics for all the forecasts and output some visuals for certain predictions versus observed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015329c7-c6ba-4215-b523-5d910adb5d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_val1a = hp.generate_series_metrics_table(clim_val_1a, clim_preds_1a, uf_name_list, \"uf_name\", disp_mode = True, normer_list = normer)\n",
    "metrics_val1b = hp.generate_series_metrics_table(clim_val_1b, clim_preds_1b, uf_name_list, \"uf_name\", disp_mode = True, normer_list = normer)\n",
    "metrics_val1c = hp.generate_series_metrics_table(clim_val_1c, clim_preds_1c, uf_name_list, \"uf_name\", disp_mode = True, normer_list = normer)\n",
    "\n",
    "metrics_val1 = hp.generate_series_metrics_table(clim_val_1, clim_preds_1, uf_name_list, \"uf_name\", disp_mode = True, normer_list = normer)\n",
    "metrics_val2 = hp.generate_series_metrics_table(clim_val_2, clim_preds_2, uf_name_list, \"uf_name\", disp_mode = True, normer_list = normer)\n",
    "metrics_val3 = hp.generate_series_metrics_table(clim_val_3, clim_preds_3, uf_name_list, \"uf_name\", disp_mode = True, normer_list = normer)\n",
    "\n",
    "metrics_targ1 = hp.generate_series_metrics_table(clim_target_1, clim_preds_targ1, uf_name_list, \"uf_name\", disp_mode = True, normer_list = normer)\n",
    "metrics_targ2 = hp.generate_series_metrics_table(clim_target_2, clim_preds_targ2, uf_name_list, \"uf_name\", disp_mode = True, normer_list = normer)\n",
    "metrics_targ3 = hp.generate_series_metrics_table(clim_target_3, clim_preds_targ3, uf_name_list, \"uf_name\", disp_mode = True, normer_list = normer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e40837-aedb-4b7e-bba7-22cd80a6c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_dfs = [metrics_val1a, metrics_val1b, metrics_val1c, metrics_val1, metrics_val2, metrics_val3, metrics_targ1, metrics_targ2, metrics_targ3]\n",
    "error_sheet_names = [\"Val1a\", \"Val1b\", \"Val1c\", \"Val1\", \"Val2\", \"Val3\", \"Targ1\", \"Targ2\", \"Targ3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e4be06-4b7b-4a2b-89b2-ef44179fccfe",
   "metadata": {},
   "source": [
    "# 6. Outputting Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f8bb0c-eca0-4aa1-b1c3-fa81353b83a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forecast_dict = {\n",
    "    \"clim_preds_1a\": clim_preds_1a, \n",
    "    \"clim_preds_1b\": clim_preds_1b, \n",
    "    \"clim_preds_1c\": clim_preds_1c, \n",
    "    \"clim_preds_1\": clim_preds_1, \n",
    "    \"clim_preds_2\": clim_preds_2, \n",
    "    \"clim_preds_3\": clim_preds_3, \n",
    "    \"clim_preds_targ1\": clim_preds_targ1, \n",
    "    \"clim_preds_targ2\": clim_preds_targ2, \n",
    "    \"clim_preds_targ3\": clim_preds_targ3, \n",
    "}\n",
    "\n",
    "forecast_dict_log = {\n",
    "    \"clim_preds_1a\": clim_preds_1a_log, \n",
    "    \"clim_preds_1b\": clim_preds_1b_log, \n",
    "    \"clim_preds_1c\": clim_preds_1c_log, \n",
    "    \"clim_preds_1\": clim_preds_1_log, \n",
    "    \"clim_preds_2\": clim_preds_2_log, \n",
    "    \"clim_preds_3\": clim_preds_3_log, \n",
    "    \"clim_preds_targ1\": clim_preds_targ1_log, \n",
    "    \"clim_preds_targ2\": clim_preds_targ2_log, \n",
    "    \"clim_preds_targ3\": clim_preds_targ3_log, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9f6e53-f7bd-4e9a-a871-89a998004b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.df_list_to_excel(error_dfs, os.path.join(forecast_res_dir, \"Errors.xlsx\"), sheet_names = error_sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18456790-46ac-4ca8-9ae3-e6eb54c5d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.save_pickle(forecast_dict, os.path.join(forecast_res_dir, \"forecast_dict.pkl\"))\n",
    "hp.save_pickle(forecast_dict_log, os.path.join(forecast_res_dir, \"forecast_dict_log.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd84a66-dc8a-4060-af17-7859aa676070",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ts_to_df(curr_ts, labels, label_names):\n",
    "    \"\"\"\n",
    "    Converts a TimeSeries into a DataFrame and labels it\n",
    "    \"\"\"\n",
    "    curr_df = curr_ts.to_dataframe()\n",
    "    time_ax_name = curr_df.index.name\n",
    "    curr_df = curr_df.reset_index()\n",
    "    comp_names = list(curr_ts.components)\n",
    "    for curr_label, curr_label_name in zip(labels, label_names):\n",
    "        curr_df[curr_label_name] = curr_label\n",
    "        \n",
    "    curr_df[[time_ax_name] + label_names + comp_names]\n",
    "    return curr_df\n",
    "\n",
    "def ts_list_to_df(curr_ts_list, labels, label_names):\n",
    "    \"\"\"\n",
    "    Converts list of TimeSeries into a single DataFrame with proper label columns\n",
    "\n",
    "    Parameters:\n",
    "    curr_ts_list (list): List of TimeSeries to convert into a DataFrame\n",
    "    labels (list): List of lists, where each component list has the same length as curr_ts_list. These are then used to label rows from the generated DataFrame.\n",
    "    label_names (list): List of label names (should be same length as labels)\n",
    "    \"\"\"\n",
    "    builder = [] \n",
    "    for ind, curr_ts in enumerate(curr_ts_list):\n",
    "        curr_labels = [curr_list[ind] for curr_list in labels]\n",
    "        curr_df = ts_to_df(curr_ts, curr_labels, label_names)\n",
    "        builder.append(curr_df)\n",
    "\n",
    "    comp_names = list(curr_ts_list[0].components)\n",
    "    time_ax_name = curr_ts_list[0].time_index.name\n",
    "    to_ret = pd.concat(builder)\n",
    "    to_ret = to_ret[[time_ax_name] + label_names + comp_names]\n",
    "    return to_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d403a4c-45ec-41a2-8c4b-404e27673cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_preds_1a_df = ts_list_to_df(clim_preds_1a, [uf_list, uf_name_list], [\"uf\", \"uf_names\"])\n",
    "clim_preds_1b_df = ts_list_to_df(clim_preds_1b, [uf_list, uf_name_list], [\"uf\", \"uf_names\"])\n",
    "clim_preds_1c_df = ts_list_to_df(clim_preds_1c, [uf_list, uf_name_list], [\"uf\", \"uf_names\"])\n",
    "\n",
    "clim_preds_1_df = ts_list_to_df(clim_preds_1, [uf_list, uf_name_list], [\"uf\", \"uf_names\"])\n",
    "clim_preds_2_df = ts_list_to_df(clim_preds_2, [uf_list, uf_name_list], [\"uf\", \"uf_names\"])\n",
    "clim_preds_3_df = ts_list_to_df(clim_preds_3, [uf_list, uf_name_list], [\"uf\", \"uf_names\"])\n",
    "\n",
    "clim_preds_targ1_df = ts_list_to_df(clim_preds_targ1, [uf_list, uf_name_list], [\"uf\", \"uf_names\"])\n",
    "clim_preds_targ2_df = ts_list_to_df(clim_preds_targ2, [uf_list, uf_name_list], [\"uf\", \"uf_names\"])\n",
    "clim_preds_targ3_df = ts_list_to_df(clim_preds_targ3, [uf_list, uf_name_list], [\"uf\", \"uf_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5297db-460d-48e4-b79b-9a31111cdf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_preds_1a_df_log = ts_list_to_df(clim_preds_1a_log, [uf_list, uf_name_list], [\"uf\", \"uf_names\"])\n",
    "clim_preds_1b_df_log = ts_list_to_df(clim_preds_1b_log, [uf_list, uf_name_list], [\"uf\", \"uf_names\"])\n",
    "clim_preds_1c_df_log = ts_list_to_df(clim_preds_1c_log, [uf_list, uf_name_list], [\"uf\", \"uf_names\"])\n",
    "\n",
    "clim_preds_1_df_log = ts_list_to_df(clim_preds_1_log, [uf_list, uf_name_list], [\"uf\", \"uf_names\"])\n",
    "clim_preds_2_df_log = ts_list_to_df(clim_preds_2_log, [uf_list, uf_name_list], [\"uf\", \"uf_names\"])\n",
    "clim_preds_3_df_log = ts_list_to_df(clim_preds_3_log, [uf_list, uf_name_list], [\"uf\", \"uf_names\"])\n",
    "\n",
    "clim_preds_targ1_df_log = ts_list_to_df(clim_preds_targ1_log, [uf_list, uf_name_list], [\"uf\", \"uf_names\"])\n",
    "clim_preds_targ2_df_log = ts_list_to_df(clim_preds_targ2_log, [uf_list, uf_name_list], [\"uf\", \"uf_names\"])\n",
    "clim_preds_targ3_df_log = ts_list_to_df(clim_preds_targ3_log, [uf_list, uf_name_list], [\"uf\", \"uf_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6a52e-8921-48d5-9273-0cef879e849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dfs = [clim_preds_1a_df, clim_preds_1b_df, clim_preds_1c_df, \n",
    "             clim_preds_1_df, clim_preds_2_df, clim_preds_3_df, \n",
    "             clim_preds_targ1_df, clim_preds_targ2_df, clim_preds_targ3_df]\n",
    "\n",
    "preds_dfs_log = [clim_preds_1a_df_log, clim_preds_1b_df_log, clim_preds_1c_df_log, \n",
    "             clim_preds_1_df_log, clim_preds_2_df_log, clim_preds_3_df_log, \n",
    "             clim_preds_targ1_df_log, clim_preds_targ2_df_log, clim_preds_targ3_df_log]\n",
    "\n",
    "\n",
    "preds_sheet_names = [\"Val1a\", \"Val1b\", \"Val1c\", \"Val1\", \"Val2\", \"Val3\", \"Targ1\", \"Targ2\", \"Targ3\"]\n",
    "\n",
    "hp.df_list_to_excel(preds_dfs, os.path.join(forecast_res_dir, \"Predictions.xlsx\"), sheet_names = preds_sheet_names)\n",
    "hp.df_list_to_excel(preds_dfs_log, os.path.join(forecast_res_dir, \"PredictionsLog.xlsx\"), sheet_names = preds_sheet_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
