{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2104a0-e0b7-4980-b771-5e57f9e755ca",
   "metadata": {},
   "source": [
    "# 1_DataProcessing\n",
    "In this notebook, we first use R to read in and explore the data from Brazil. The goal is to decide on the what parts of the data to use, and to format everything for easy reading into Python (for the creation of Darts TS objects). Part of this is also deciding on the training-validation-testing regimen to use.\n",
    "\n",
    "There are three steps to this process:\n",
    "1. Compute state-level dengue case data (provided values are at the municipality-level and need to be aggregated).\n",
    "2. Compute state-level static covariates (we can use the proportion of each state living in each climate zone / biome).\n",
    "3. Compute state-level meteorological values (municipality values provided but we can aggregate to state-level through population weighting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8797ac-e37e-4103-86c0-89ccc20adc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae85efdf-0a68-4a5a-b11b-7f9977b32151",
   "metadata": {},
   "source": [
    "# 1. Reading Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16049d19-af4d-432e-902d-555ceff3d28e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_dir = getwd()\n",
    "raw_data_dir = file.path(base_dir, \"data_sprint_2025\")\n",
    "api_data_dir = file.path(base_dir, \"API Downloaded\")\n",
    "other_data_dir = file.path(base_dir, \"Other Data\")\n",
    "output_dir = file.path(base_dir, \"ModelInput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010f384a-97d0-4559-bd85-16da65ff2611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dengue_df = read_csv(file.path(raw_data_dir, \"dengue.csv.gz\")) #Dengue data\n",
    "geo_df = read_csv(file.path(raw_data_dir, \"map_regional_health.csv\")) #Information on geocodes and admin hierarchy\n",
    "pop_df = read_csv(file.path(raw_data_dir, \"datasus_population_2001_2024.csv.gz\")) #Population data\n",
    "environ_df = read_csv(file.path(raw_data_dir, \"environ_vars.csv.gz\")) #Environmental variables (used for statis covariates). \n",
    "climate_df = read_csv(file.path(raw_data_dir, \"climate.csv.gz\")) #Municipality-level climate covariates\n",
    "api_dl_clim_df = read_csv(file.path(api_data_dir, \"climate_2025.csv\")) #Municipality-level API-downloaded climate data (dfiff format from climate_df)\n",
    "\n",
    "covid_index_df = read_csv(file.path(other_data_dir, \"OxCGRT_compact_subnational_v1.csv\"))\n",
    "\n",
    "sst_df = read_csv(file.path(raw_data_dir, \"ocean_climate_oscillations.csv.gz\")) %>% rename(Date = date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1067f674-a03e-4e24-935d-f1c81e612338",
   "metadata": {},
   "source": [
    "# 2. Processing Case Data\n",
    "Here, we aggregate case data to the state-level and output a long format file that can easily be used to create Darts TimeSeries objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e9647-d57e-43f2-8085-582cc1424c46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cases_df = dengue_df %>% mutate(Year = as.integer(substr(epiweek, 1, 4)), Week = as.integer(substr(epiweek, 5, 6))) %>% \n",
    "    select(date, epiweek, Year, Week, uf, casos, train_1, target_1, train_2, target_2, train_3, target_3) %>% \n",
    "    group_by(date, epiweek, Year, Week, uf) %>% summarise(casos = sum(casos), train_1 = all(train_1), target_1 = all(target_1),\n",
    "                                                train_2 = all(train_2), target_2 = all(target_2), train_3 = all(train_3), target_3 = all(target_3)) %>% ungroup\n",
    "\n",
    "state_mapper = geo_df %>% select(uf, uf_name) %>% distinct\n",
    "cases_df = cases_df %>% left_join(state_mapper, by = \"uf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a993ab7e-9bcc-4b03-9dc6-09fa8f90ac5e",
   "metadata": {},
   "source": [
    "# 3. State-level Static Covariates\n",
    "We can compute state-level static covariates by computing the percentage of each state population living in each Koppen climate class/biome. We can base this on the mid-study period population to keep this as a static covariate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32230a58-ae36-4916-93a9-40d019ff7cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mun_to_state = geo_df %>% select(geocode, uf, uf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83baa51b-f33a-4794-8e38-568fa66419e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mun_pop_df = pop_df %>% left_join(mun_to_state, by = \"geocode\")\n",
    "state_pop_df = mun_pop_df\n",
    "state_pop_df = state_pop_df %>% select(uf, uf_name, year, population) %>% group_by(uf, uf_name, year) %>% summarise(population = sum(population)) %>% \n",
    "                rename(uf_population = population)\n",
    "\n",
    "mun_pop_df = mun_pop_df %>% left_join(state_pop_df, by = c(\"year\", \"uf\", \"uf_name\")) %>% mutate(mun_uf_prop = population / uf_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cd6a04-061a-467d-b7e2-af3dbbf32b71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uf_list = mun_pop_df %>% pull(uf) %>% unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847b8de-c875-4932-8220-2696f04fadef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Format the environmental static covariates\n",
    "mun_pop_year = as.integer((2010 + 2024) / 2)\n",
    "mun_pop_filt = mun_pop_df %>% filter(year == mun_pop_year)\n",
    "\n",
    "stat_cov_df = environ_df %>% left_join(mun_pop_filt, by = \"geocode\") %>% \n",
    "                mutate(biome = ifelse(biome == \"Mata AtlÃ¢ntica\", \"Mata Atlantica\",\n",
    "                                     ifelse(biome == \"AmazÃ´nia\", \"Amazonia\", biome)))\n",
    "koppen_df = stat_cov_df %>% select(uf, uf_name, koppen, mun_uf_prop) %>% group_by(uf, uf_name, koppen) %>% \n",
    "            summarise(mun_uf_prop = sum(mun_uf_prop)) %>% mutate(koppen = paste0(\"koppen_\", koppen)) %>% \n",
    "            pivot_wider(id_cols = c(uf, uf_name), names_from = koppen, values_from = mun_uf_prop) %>% \n",
    "            mutate(across(everything(), ~replace_na(., 0)))\n",
    "\n",
    "biome_df = stat_cov_df %>% select(uf, uf_name, biome, mun_uf_prop) %>% group_by(uf, uf_name, biome) %>% \n",
    "            summarise(mun_uf_prop = sum(mun_uf_prop)) %>% mutate(biome = paste0(\"biome_\", biome)) %>% \n",
    "            pivot_wider(id_cols = c(uf, uf_name), names_from = biome, values_from = mun_uf_prop) %>% \n",
    "            mutate(across(everything(), ~replace_na(., 0)))\n",
    "merged_stats_df = koppen_df %>% left_join(biome_df, by = c(\"uf\", \"uf_name\"))\n",
    "merged_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4985b15-0b6d-4ce3-af12-159339ac83e1",
   "metadata": {},
   "source": [
    "# 4. Processing Meteorological Values\n",
    "We generate state-level meteorological values coming from the municipality-level values. To do this, we apply a population weighting scheme - ensuring each value is weighted by the proportion of the state population living in the municipality in the given year. We assume that the population proportions for 2025 are the same as in 2024. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f311d60-5eb9-4529-8f49-2369f81b7346",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mun_pop_2024 = mun_pop_df %>% filter(year == 2024)\n",
    "mun_weights = mun_pop_df %>% rbind(mun_pop_2024 %>% mutate(year = 2025)) %>% #Assume that the proportions and populations in 2025 are the same as in 2024.\n",
    "                rename(Year = year) %>% \n",
    "                select(Year, geocode, uf, uf_name, mun_uf_prop)\n",
    "mun_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdcaadd-a7e3-437f-9813-37ed3d4b7124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "api_dl_clim_df_cleaned = api_dl_clim_df %>% select(-`...1`) %>% \n",
    "                                            rename(pressure_min = pressao_min, pressure_med = pressao_med, pressure_max = pressao_max,\n",
    "                                             rel_humid_min = umid_min, rel_humid_med = umid_med, rel_humid_max = umid_max, geocode = geocodigo) %>% #Rename columns to match original climate data provided by organisers\n",
    "                                             mutate(rainy_days = ifelse(precip_tot > 0.03, 1, 0), num_days = 1) %>%  #Set rainy days definition based on information from organisers\n",
    "                                            filter(epiweek != 202501)\n",
    "\n",
    "#Aggregate weekly values\n",
    "api_dl_clim_df_cleaned_weekly = api_dl_clim_df_cleaned %>% group_by(geocode, epiweek) %>% \n",
    "                                    summarise(date = min(date), temp_min = mean(temp_min), temp_med = mean(temp_med), temp_max = mean(temp_max),\n",
    "                                             precip_min = sum(precip_min), precip_med = sum(precip_med), precip_max = sum(precip_max),\n",
    "                                             pressure_min = mean(pressure_min), pressure_med = mean(pressure_med), pressure_max = mean(pressure_max),\n",
    "                                             rel_humid_min = mean(rel_humid_min), rel_humid_med = mean(rel_humid_med), rel_humid_max = mean(rel_humid_max),\n",
    "                                             rainy_days = sum(rainy_days), num_days = sum(num_days)) %>% mutate(thermal_range = temp_max - temp_min)\n",
    "api_dl_clim_df_cleaned_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a15590c-d1e9-4264-bbcf-738cac7e0dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that the API downloaded climate data has been processed in the same way as the original provided by organisers.\n",
    "temp1 = api_dl_clim_df_cleaned_weekly %>% filter(epiweek == 202515) %>% arrange(geocode) %>% select(-num_days)\n",
    "temp1 = temp1[, sort(names(temp1))]\n",
    "temp2 = climate_df %>% filter(epiweek == 202515) %>% arrange(geocode)\n",
    "temp2 = temp1[, sort(names(temp2))]\n",
    "identical(temp1, temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e74849-18ed-4ddd-8733-6552162a473d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We get a collection of the municipalities where climate data is not available. These are primarily islands. \n",
    "climate_cleaned_df = climate_df\n",
    "\n",
    "orig_end = climate_cleaned_df %>% pull(epiweek) %>% max #Last epiweek given by the organisers\n",
    "add_from_api = api_dl_clim_df_cleaned_weekly %>% filter(epiweek > orig_end) %>% select(-num_days) #Get epiweeks from the API that come after the data provided by organisers\n",
    "\n",
    "climate_cleaned_df = climate_cleaned_df %>% rbind(add_from_api) #Merge the data from the API into the climate DataFrame\n",
    "\n",
    "clim_avail_codes = climate_cleaned_df %>% pull(geocode) %>% unique\n",
    "missing_codes = mun_weights %>% filter(!(geocode %in% clim_avail_codes)) %>% pull(geocode) %>% unique\n",
    "\n",
    "#The missing geocodes do not have any climate data. To solve this, we take climate data for some nearby municipality, change the geocode, then add it to our climate DataFrame\n",
    "\n",
    "#2916104 Itaparica is assigned 2933208 Vera Cruz\n",
    "#2919926 Madre de Deus is assigned to 2929206 Sao Francisco do Conde\n",
    "#2605459 Fernando de Noronha = 2407500 Maxaranguape\n",
    "missing_filler = list(\"2916104\" = \"2933208\", \"2919926\" = \"2929206\", \"2605459\" = \"2407500\")\n",
    "\n",
    "#Use the climate in another municiaplit to fill in the values for those that are missing\n",
    "builder = list()\n",
    "missing_list = names(missing_filler)\n",
    "for(curr_missing in missing_list){\n",
    "    #Retrieve values based on missing_filler\n",
    "    to_fill = climate_cleaned_df %>% filter(geocode == as.integer(missing_filler[[curr_missing]]))\n",
    "    to_fill = to_fill %>% mutate(geocode = as.integer(curr_missing)) #Change the geocode to what is missing\n",
    "    builder[[curr_missing]] = to_fill #add to the builder list\n",
    "}\n",
    "clim_addition = do.call(rbind, builder)\n",
    "rownames(clim_addition) = NULL\n",
    "\n",
    "climate_cleaned_df = climate_cleaned_df %>% rbind(clim_addition)\n",
    "\n",
    "#Below is info on the missing geocodes\n",
    "#geo_df %>% filter(geocode %in% missing_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4d708-7fe6-451c-bd61-c8442d6b4052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clim_weighted_df = climate_cleaned_df %>% \n",
    "                mutate(Year = as.integer(substr(epiweek, 1, 4)), Week = as.integer(substr(epiweek, 5, 6))) %>% #Split epiweek into Year Week\n",
    "                filter(Year >= 2010) %>% #We only get the values from 2010 since those are the years we have dengue data\n",
    "                left_join(mun_weights, by = c(\"Year\", \"geocode\"))  %>% \n",
    "                rename(Date = date, weight = mun_uf_prop) %>% \n",
    "                mutate(across(\n",
    "                    .cols = -c(Date, epiweek, geocode, Year, Week, uf, uf_name, weight),\n",
    "                    .fns = ~.x * weight\n",
    "                ))\n",
    "clim_weighted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9d443-548d-452d-82f8-5db320da6674",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_clim_df = clim_weighted_df %>% #Get the weighted DataFrame\n",
    "                select(Date, epiweek, Year, Week, uf, uf_name, everything()) %>% #Re-order the columns\n",
    "                select(-c(geocode)) %>% #Remove municipality codes\n",
    "                group_by(Date, epiweek, Year, Week, uf, uf_name) %>% #Group by and summarise to get the weighted sum\n",
    "                summarise(across(everything(), sum)) %>% \n",
    "                mutate(thermal_range = temp_max - temp_min) #Recompute thermal_range to be sure\n",
    "state_clim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe86210a-cce9-4068-a41b-379afe7cd2a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check if there is a state where the sum of weights is not 1 for any date. This should be empty\n",
    "checker1 = state_clim_df %>% filter((weight + 0.000001) < 1) %>% nrow  #We use a small epsilon due to some precision issues\n",
    "weights_check = mun_weights %>% filter(Year >= 2010) %>% select(-geocode) %>% group_by(Year, uf, uf_name) %>% summarise(mun_uf_prop = sum(mun_uf_prop))\n",
    "checker2 = weights_check %>% filter((mun_uf_prop + 0.00001) < 1) %>% nrow\n",
    "\n",
    "(checker1 == 0) & (checker2==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f907377-2357-4d1f-aa61-04d17c02cb4f",
   "metadata": {},
   "source": [
    "# 5. Processing COVID-19 Stringency Index\n",
    "We use the sub-national level stringency index from the Oxford COVID-19 Government Response Tracker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af99874-bc44-4ca3-951f-5c153aa8f273",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generate a DataFrame with the WeekStart, WeekMid, WeekEnd information. \n",
    "week_cal = climate_cleaned_df %>% select(date, epiweek) %>% distinct %>% \n",
    "                rename(WeekStart = date) %>% mutate(WeekEnd = WeekStart + 6) %>% \n",
    "                mutate(WeekMid = WeekStart + 4) %>% \n",
    "                select(epiweek, WeekStart, WeekMid, WeekEnd)\n",
    "week_cal \n",
    "\n",
    "covid_brazil = covid_index_df %>% filter(CountryName == \"Brazil\")\n",
    "\n",
    "covid_state_df = covid_brazil %>% filter(Jurisdiction == \"STATE_TOTAL\") %>% select(RegionName, RegionCode, Jurisdiction, Date, StringencyIndex_Average) %>% \n",
    "                    mutate(Date = as.Date(as.character(Date), format = \"%Y%m%d\")) %>% \n",
    "                    mutate(RegionCode = str_replace(RegionCode, \"BR_\", \"\")) %>% select(-Jurisdiction) %>% \n",
    "                    rename(StringencyIndex = StringencyIndex_Average)\n",
    "covid_state_df = covid_state_df %>% left_join(week_cal, by = join_by(between(Date, WeekStart, WeekEnd)))\n",
    "\n",
    "covid_state_df\n",
    "\n",
    "covid_state_df %>% filter(RegionCode == \"SP\")\n",
    "\n",
    "#Get weekly average stringency index \n",
    "weekly_covid_state_df = covid_state_df %>% mutate(num_days = 1) %>% select(-Date) %>%\n",
    "                        group_by(RegionName, RegionCode, epiweek, WeekStart, WeekMid, WeekEnd) %>% \n",
    "                        summarise(num_days = sum(num_days), StringencyIndex = mean(StringencyIndex)) %>% ungroup\n",
    "weekly_covid_state_df = weekly_covid_state_df %>% rename(uf = RegionCode, uf_name = RegionName) %>% \n",
    "                        mutate(Year = str_sub(epiweek, 1, 4), Week = str_sub(epiweek, 5, 6)) %>% \n",
    "                        mutate(Year = as.integer(Year), Week = as.integer(Week))\n",
    "weekly_covid_state_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef3c8c-5348-43c1-82fa-dc94f40c778b",
   "metadata": {},
   "source": [
    "# 6. Processing SST Indices\n",
    "We also process the SST Indices so we can look into potentially forecasting them and using them as covariates to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948ebea7-0d1f-4d43-b2a3-aca5737a5b82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width = 12, repr.plot.height = 5)\n",
    "\n",
    "sst_df %>% pivot_longer(-Date, names_to = \"IndexName\", values_to = \"Value\") %>%\n",
    "    ggplot(aes(x = Date, y = Value)) + \n",
    "        geom_point() + \n",
    "        theme(text = element_text(size = 14)) + \n",
    "        facet_wrap(~IndexName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309b5021-b625-446d-aace-1e0252f872b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interp_date_start = sst_df %>% pull(Date) %>% min\n",
    "interp_date_end = sst_df %>% pull(Date) %>% max\n",
    "\n",
    "\n",
    "#Date range to generate daily values for\n",
    "to_interp_dates = seq(from = interp_date_start, to = interp_date_end, by = \"day\")\n",
    "\n",
    "#Use linear interpolation to fill in gaps in the SST indices\n",
    "enso_approx = approx(x = sst_df %>% pull(Date), y = sst_df %>% pull(enso), xout = to_interp_dates)\n",
    "iod_approx = approx(x = sst_df %>% pull(Date), y = sst_df %>% pull(iod), xout = to_interp_dates)\n",
    "pdo_approx = approx(x = sst_df %>% pull(Date), y = sst_df %>% pull(pdo), xout = to_interp_dates)\n",
    "\n",
    "sst_filled_df = data.frame(Date = to_interp_dates, enso = enso_approx$y, iod = iod_approx$y, pdo = pdo_approx$y, num_days = 1) \n",
    "\n",
    "weekly_sst_df = sst_filled_df%>% left_join(week_cal, by = join_by(between(Date, WeekStart, WeekEnd))) %>% #Join to calendar\n",
    "                    filter(!is.na(epiweek)) %>% select(-Date) %>% #Remove those with no epiweek (outside study period) and remove date column\n",
    "                    group_by(epiweek, WeekStart, WeekMid, WeekEnd) %>% \n",
    "                    summarise(enso = mean(enso), iod = mean(iod), pdo = mean(pdo), num_days = sum(num_days)) %>% #Get average values\n",
    "                    mutate(Year = str_sub(epiweek, 1, 4), Week = str_sub(epiweek, 5, 6)) %>% \n",
    "                    mutate(Year = as.integer(Year), Week = as.integer(Week)) #Split the Year and Week values\n",
    "\n",
    "\n",
    "weekly_sst_df = weekly_sst_df %>% select(Year, Week, epiweek, WeekStart, WeekMid, WeekEnd, enso, iod, pdo, num_days)\n",
    "weekly_sst_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e562d55-28c0-4a55-b34d-4d4e896b0e2b",
   "metadata": {},
   "source": [
    "# 7. Putting Things Together and Outputting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c5f8cc-cc45-4ab3-bda3-13eb019ebedd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Attach the COVID Stringency Index to the climate data to form the time-varying covariates dataframe\n",
    "covid_index_attach = weekly_covid_state_df %>% select(uf, Year, Week, StringencyIndex)\n",
    "time_varying_covs_df = state_clim_df %>% left_join(covid_index_attach, by = c(\"uf\", \"Year\", \"Week\")) %>% \n",
    "                        replace_na(list(StringencyIndex = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b265bd02-e26e-4d2b-a399-38c93fecc1c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_varying_covs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b672f-907c-4797-aca3-2848fea62012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_cases_df = cases_df %>% rename(Cases = casos, Date = date) %>%\n",
    "                    select(Date, epiweek, Year, Week, uf, uf_name, Cases, train_1, target_1, train_2, target_2, train_3, target_3)\n",
    "main_cases_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf83dbc-4f9d-491d-9434-59c9622cc783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calendar = week_cal %>% mutate(Year = str_sub(epiweek, 1, 4), Week = str_sub(epiweek, 5, 6)) %>% \n",
    "                        mutate(Year = as.integer(Year), Week = as.integer(Week))\n",
    "\n",
    "set_info = main_cases_df %>% select(Year, Week, train_1, target_1, train_2, target_2, train_3, target_3) %>% distinct\n",
    "\n",
    "calendar = calendar %>% left_join(set_info, by = c(\"Year\", \"Week\")) %>% \n",
    "                        replace_na(list(train_1 = FALSE, target_1 = FALSE, train_2 = FALSE, target_2 = FALSE, train_3 = FALSE, target_3 = FALSE))\n",
    "calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae3a55b-085a-4371-9ea8-98bb94231d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Write values to csv files\n",
    "# main_cases_df %>% write.csv(file.path(output_dir, \"DengueCases.csv\"), row.names = FALSE) #CSV file with cases\n",
    "# time_varying_covs_df %>% write.csv(file.path(output_dir, \"TimeVaryingCovs.csv\"), row.names = FALSE) #CSV file with time-varying covariates (climate + COVID-19 stringency index)\n",
    "# merged_stats_df %>% write.csv(file.path(output_dir, \"StaticCovs.csv\"), row.names = FALSE) #CSV file with static covariate information - Koppen climate and Brazilian biomes\n",
    "# weekly_sst_df%>% write.csv(file.path(output_dir, \"SSTIndices.csv\"), row.names = FALSE) #CSV file containing interpolated - weekly aggregated SST indices\n",
    "# calendar %>% write.csv(file.path(output_dir, \"Calendar.csv\"), row.names = FALSE) #CSV file containing epiweek calendar information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
