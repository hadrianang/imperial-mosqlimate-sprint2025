{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "351230c1-55d4-4dd9-886b-f081ae093e05",
   "metadata": {},
   "source": [
    "# 2_1_ClimateTuning\n",
    "Before we can work on forecasting and tuning with the model, we will have to generate a climate forecasting model. As part of the forecasting challenge, there is a gap between the input data and the output. Essentially, we can only use data up to EW 25 of year $t$ and we need to forecast for EW 41 year $t$ to EW40 year $t+1$. This gap includes both dengue and climate data. We try tuning three different models here: (1) linear regression, (2) random forest, (3) XGBoost, focussing only on the lags hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15b5b6a-77c7-4448-8cce-53a56031c3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import os\n",
    "import json\n",
    "\n",
    "from darts import TimeSeries, concatenate\n",
    "from darts.utils.callbacks import TFMProgressBar\n",
    "from darts.metrics import mape, smape, mse, rmse, mae, mql, ql\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.dataprocessing.transformers import StaticCovariatesTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from darts.models import (\n",
    "    RandomForestModel,\n",
    "    LinearRegressionModel,\n",
    "    XGBModel\n",
    ")\n",
    "from darts.utils.likelihood_models import GaussianLikelihood, PoissonLikelihood, QuantileRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning.callbacks import Callback, EarlyStopping\n",
    "import pickle\n",
    "pd.options.mode.copy_on_write = True\n",
    "random_seed = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1024ce-9678-49fc-a261-0c3ee17ad35c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 0. Helper Functions and Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d58875c-e087-40ea-9916-d74084f32a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_var_names = [\"temp_min\", \"temp_med\", \"temp_max\", \"precip_min\", \"precip_med\", \"precip_max\", \"pressure_min\", \"pressure_med\", \"pressure_max\", \"rel_humid_min\", \"rel_humid_med\", \"rel_humid_max\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bd978c-e74e-4d80-aa23-b7e701fc0d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mv_series(obs_ts, preds_ts, curr_fig_dims, curr_val_cols, width, height, train_ts = None, train_ts_length = None): \n",
    "    fig, axs = plt.subplots(curr_fig_dims[0], curr_fig_dims[1], figsize = (width,  height))\n",
    "    for curr_col, curr_ax in zip(curr_val_cols, axs.flatten()):\n",
    "        if train_ts is not None:\n",
    "            if train_ts_length is not None:\n",
    "                train_ts[curr_col][-train_ts_length:].plot(label = \"Training\", ax = curr_ax)\n",
    "            else:\n",
    "                train_ts[curr_col].plot(label = \"Training\", ax = curr_ax)\n",
    "        obs_ts[curr_col].plot(label = \"Observed\", ax = curr_ax)\n",
    "        preds_ts[curr_col].plot(label = \"Forecasted\", ax = curr_ax)\n",
    "        curr_ax.set_title(curr_col)\n",
    "        curr_ax.set_xlabel(\"\")\n",
    "\n",
    "\n",
    "def log_bound(x, lower, upper, plus_1 = True, eps = 1e-6):\n",
    "    \"\"\"\n",
    "    Transforms some value x such that when we forecast it, it will always be between lower and upper\n",
    "    \"\"\"\n",
    "    if plus_1: \n",
    "        x += 1\n",
    "        lower += 1\n",
    "        upper += 1\n",
    "        \n",
    "    y = np.log1p((x - lower)/(upper - x + eps))\n",
    "    return y\n",
    "\n",
    "def inv_log_bounds(y, lower, upper, plus_1 = True, eps = 1e-6):\n",
    "    \"\"\"\n",
    "    Inverse of the log_bounds transformation above\n",
    "    \"\"\"\n",
    "    exp_y = np.exp(y)\n",
    "    numerator = (exp_y - 1) * (upper + eps) + lower\n",
    "    denominator = exp_y\n",
    "    return numerator / denominator\n",
    "\n",
    "def generate_preds_obs_plot(train_list, obs_list, preds_list, uf_list, uf_sub_list, clim_var_names = clim_var_names, label_list = None, width = 20, height = 42, show_train = True):\n",
    "    to_vis_train = [curr_ts for curr_ts, curr_uf in zip(train_list, uf_list) if curr_uf in uf_sub_list]\n",
    "    to_vis_preds_list = [curr_ts for curr_ts, curr_uf in zip(preds_list, uf_list) if curr_uf in uf_sub_list]\n",
    "    to_vis_obs_list = [curr_ts for curr_ts, curr_uf in zip(obs_list, uf_list) if curr_uf in uf_sub_list]\n",
    "    fig, axs = plt.subplots(len(clim_var_names), len(uf_sub_list), figsize = (width,  height))\n",
    "    if label_list is None:\n",
    "        label_list = uf_sub_list\n",
    "    else:\n",
    "        label_list = [curr_label for curr_label, curr_uf in zip(label_list, uf_list) if curr_uf in uf_sub_list]\n",
    "    for ind1, (curr_train, curr_obs, curr_preds, curr_label) in enumerate(zip(to_vis_train, to_vis_obs_list, to_vis_preds_list, label_list)):\n",
    "        for ind2, curr_var in enumerate(clim_var_names):\n",
    "            if show_train:\n",
    "                curr_train[curr_var].plot(ax = axs[ind2][ind1], label = \"Training\")\n",
    "            curr_obs[curr_var].plot(ax = axs[ind2][ind1], label = \"Observed\")\n",
    "            curr_preds[curr_var].plot(ax = axs[ind2][ind1], label = \"Forecasted\")\n",
    "            axs[ind2][ind1].set_xlabel(\"\")\n",
    "            axs[ind2][ind1].set_ylabel(curr_var)\n",
    "            axs[ind2][ind1].set_title(curr_label)\n",
    "\n",
    "def norm_metric(curr_obs, curr_preds, curr_normer, summ_func = None, metric = mae):\n",
    "    \"\"\"\n",
    "    Computes a normalised version of some Darts metric given an observed, predicted, and normalising TimeSeries.\n",
    "    Note that this function assumes that all inputs are on their natural scales (should invert any log transforms or min-max scaling)\n",
    "\n",
    "    Parameters:\n",
    "    curr_obs (TimeSeries): Darts TimeSeries of observed values to compute metrics against\n",
    "    curr_preds (TimeSeries): Darts TimeSeries forecasted values to commpute metrics with\n",
    "    curr_normer (TimeSeries): Darts TimeSeries that serves as the basis of min-max values to normalise metrics (usually the training set)\n",
    "    summ_func (function): Function to summarise the normalised per component metrics\n",
    "    metric (function): Darts metric function (see: https://unit8co.github.io/darts/generated_api/darts.metrics.metrics.html)\n",
    "\n",
    "    Returns:\n",
    "    Value of the computed metric summarised (if summ_func is not None) or an array containing the metric per component (if summ_func is None)\n",
    "    \"\"\"\n",
    "    curr_preds = curr_preds.slice_intersect(curr_obs) #Get the part of the forecast that intersects with the observation\n",
    "    assert len(curr_preds) == len(curr_obs) #Check that the forecasts and observations match in length\n",
    "    \n",
    "    bounds_list = [] \n",
    "    for comp in curr_normer.components: #Use the normer series to find the min and max per component\n",
    "        values = curr_normer[comp].values()\n",
    "        comp_min = values.min()\n",
    "        comp_max = values.max()\n",
    "        bounds_list.append((comp_min, comp_max))\n",
    "    \n",
    "    met_per_comp = metric(curr_obs, curr_preds, component_reduction = None) #Get the metric value per component\n",
    "    norm_met_vals = []\n",
    "    for met, min_max in zip(met_per_comp, bounds_list):\n",
    "        curr_min = min_max[0]\n",
    "        curr_max = min_max[1]\n",
    "        norm_met_vals.append(met / (curr_max - curr_min)) #Normalise the component error based on the min and max for each component\n",
    "    norm_met_vals = np.array(norm_met_vals)\n",
    "\n",
    "    #Whether to summarise the metric values or to just return the whole array\n",
    "    if summ_func is None:\n",
    "        return norm_met_vals\n",
    "    else:\n",
    "        return summ_func(norm_met_vals)\n",
    "\n",
    "def norm_metric_list(obs_list, preds_list, normer_list, comp_summ_func = None, series_summ_func = None, metric = mae):\n",
    "    \"\"\"\n",
    "    Computes a normalised metric using a list of observed, predicted, and normalising series. Function assumes that all series are on\n",
    "    their natural scales. \n",
    "\n",
    "    Parameters:\n",
    "    obs_list (list): List of Darts TimeSeries containing observed values to compute metrics against\n",
    "    preds_list (list): List of Darts TimeSeries containing forecasted values to commpute metrics with\n",
    "    normer_list (list): List of Darts TimeSeries that serve as the basis of min-max values to normalise metrics (usually the training set)\n",
    "    comp_summ_func (function): Function to summarise the normalised per component metrics\n",
    "    series_summ_func (function): Function to summarise metrics across different series\n",
    "    metric (function): Darts metric function (see: https://unit8co.github.io/darts/generated_api/darts.metrics.metrics.html)\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for curr_obs, curr_preds, curr_normer in zip(obs_list, preds_list, normer_list):\n",
    "        met_val = norm_metric(curr_obs, curr_preds, curr_normer, summ_func = comp_summ_func, metric = metric)\n",
    "        result.append(met_val)\n",
    "    result = np.array(result)\n",
    "\n",
    "    if series_summ_func is None:\n",
    "        return result\n",
    "    else:\n",
    "        return series_summ_func(result)\n",
    "\n",
    "#def generate_component_metrics_table(obs_list, preds_list, label_list, label_name, component_names, normer_list = None, metrics = [mae], metric_names = [\"MAE\"], disp_mode = False):\n",
    "def generate_component_metric_table(obs_list, preds_list, label_list, label_name, component_names, curr_metric, normer_list = None, disp_mode = False):\n",
    "    \"\"\"\n",
    "    Function generates a table meant to display the error per state per component\n",
    "    \"\"\"\n",
    "    table_builder = []\n",
    "    if normer_list is not None: \n",
    "        metric_res = norm_metric_list(obs_list, preds_list, normer_list, metric = curr_metric)\n",
    "    else:\n",
    "        metric_res = curr_metric(obs_list, preds_list, component_reduction = None)\n",
    "    \n",
    "    for curr_label, curr_errors in zip(label_list, metric_res):\n",
    "        curr_dict = {label_name: curr_label}\n",
    "        for curr_comp, curr_comp_error in zip(component_names, curr_errors):\n",
    "            curr_dict[curr_comp] = curr_comp_error\n",
    "        table_builder.append(curr_dict)\n",
    "        \n",
    "    curr_table = pd.DataFrame(table_builder)\n",
    "    \n",
    "    if disp_mode:\n",
    "        disp_table = curr_table.copy()\n",
    "        summ_row = {label_name: \"Summary\"}\n",
    "        for curr_comp in component_names:\n",
    "            central = np.mean(curr_table[curr_comp])\n",
    "            lower = np.min(curr_table[curr_comp])\n",
    "            upper = np.max(curr_table[curr_comp])\n",
    "            summ_row[curr_comp] = \"%.3f (%.3f to %.3f)\" % (central, lower, upper)\n",
    "            disp_table[curr_comp] = disp_table.apply(lambda row: \"%.3f\" % row[curr_comp], axis = 1)\n",
    "        disp_table = pd.concat([disp_table, pd.DataFrame([summ_row])])\n",
    "        return disp_table\n",
    "    \n",
    "    return curr_table\n",
    "\n",
    "def generate_series_metrics_table(obs_list, preds_list, label_list, label_name, metrics = [mae, rmse], metric_names = [\"MAE\", \"RMSE\"],  normer_list = None, disp_mode = False):\n",
    "    \"\"\"\n",
    "    Function displays multiple error metrics (each the mean across components) per state\n",
    "    \"\"\"\n",
    "    if normer_list is not None:\n",
    "        metric_names = [\"Norm_\" + curr_name for curr_name in metric_names]\n",
    "    curr_table = pd.DataFrame([{label_name: curr_label} for curr_label in label_list])\n",
    "    for curr_metric, curr_metric_name in zip(metrics, metric_names):\n",
    "        if normer_list is not None: \n",
    "            curr_res = norm_metric_list(obs_list, preds_list, normer_list, comp_summ_func = np.mean, metric = curr_metric)\n",
    "        else:\n",
    "            curr_res = metric(obs_list, preds_list)\n",
    "        curr_table[curr_metric_name] = curr_res\n",
    "    \n",
    "    if disp_mode:\n",
    "        disp_table = curr_table.copy()\n",
    "        summ_row = {label_name: \"Summary\"}\n",
    "        for curr_metric_name in metric_names:\n",
    "            central = np.mean(curr_table[curr_metric_name])\n",
    "            lower = np.min(curr_table[curr_metric_name])\n",
    "            upper = np.max(curr_table[curr_metric_name])\n",
    "            summ_row[curr_metric_name] = \"%.3f (%.3f to %.3f)\" % (central, lower, upper)\n",
    "            disp_table[curr_metric_name] = disp_table.apply(lambda row: \"%.3f\" % row[curr_metric_name], axis = 1)\n",
    "            \n",
    "        disp_table = pd.concat([disp_table, pd.DataFrame([summ_row])])\n",
    "        return disp_table\n",
    "        \n",
    "    return curr_table\n",
    "\n",
    "def dict_as_json(curr_json, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(curr_json, f, indent = 4)\n",
    "\n",
    "def save_pickle(to_save, save_path):\n",
    "    with open(save_path, \"wb\") as file:\n",
    "        pickle.dump(to_save, file)\n",
    "\n",
    "def load_pickle(load_path):\n",
    "    to_ret = None \n",
    "    with open(load_path, \"rb\") as file:\n",
    "        to_ret = pickle.load(file)\n",
    "    return(to_ret)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2986d93-fc63-4941-bd17-4b9f42714b5c",
   "metadata": {},
   "source": [
    "## 1. Reading Input\n",
    "First, we read in the input files: (1) a weekly calendar - which also includes flags for inclusion in the different data sets, (2) SST Indices, (3) Climate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b45c8-cd37-4df0-a489-18553fedd2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_scale = False\n",
    "retrain_per_fold = True\n",
    "approach = \"forecast_gap\"\n",
    "\n",
    "tuning_name = \"Notebook_24_07_2025\"\n",
    "base_dir = os.getcwd()\n",
    "model_input_dir = os.path.join(base_dir, \"ModelInput\")\n",
    "tuning_dir = os.path.join(base_dir, \"Climate Tuning\", tuning_name)\n",
    "\n",
    "os.makedirs(tuning_dir, exist_ok = True)\n",
    "\n",
    "tuning_details = {\"clim_var_names\": clim_var_names, \"global_scale\": global_scale, \"retrain_per_fold\": retrain_per_fold, \"approach\": approach}\n",
    "dict_as_json(tuning_details, os.path.join(tuning_dir, \"tuning_details.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc99c7e9-b688-4ccf-9560-52322a2cb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_cal = pd.read_csv(os.path.join(model_input_dir, \"Calendar.csv\"))\n",
    "sst_df = pd.read_csv(os.path.join(model_input_dir, \"SSTIndices.csv\"))\n",
    "climate_df = pd.read_csv(os.path.join(model_input_dir, \"TimeVaryingCovs.csv\"))\n",
    "\n",
    "#Remove uf = \"ES\" since there is no dengue data for Espirito Santo\n",
    "climate_df = climate_df[climate_df[\"uf\"] != \"ES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e9ae3f-de2f-456f-a1c5-ebba2b0f6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set info where validation sets in the main dengue forecasting part occur only during the dengue season W41-40 year t to t+1. \n",
    "set_info = weekly_cal[[\"epiweek\", \"Year\", \"Week\", \"WeekStart\", \"WeekMid\", \"WeekEnd\"]]\n",
    "set_info = set_info.iloc[1:]\n",
    "\n",
    "#Define the data sets using a column of booleans in the calendar. \n",
    "#all epiweek numbers are in YYYYWW integer format\n",
    "set_info[\"clim_train1\"] = set_info[\"epiweek\"] <= 201510\n",
    "set_info[\"clim_val1\"] = (set_info[\"epiweek\"] >= 201526) & (set_info[\"epiweek\"] <= 201625)\n",
    "set_info[\"clim_train2\"] = set_info[\"epiweek\"] <= 201610\n",
    "set_info[\"clim_val2\"] = (set_info[\"epiweek\"] >= 201626) & (set_info[\"epiweek\"] <= 201725)\n",
    "set_info[\"clim_train3\"] = set_info[\"epiweek\"] <= 201710\n",
    "set_info[\"clim_val3\"] = (set_info[\"epiweek\"] >= 201726) & (set_info[\"epiweek\"] <= 201825)\n",
    "set_info[\"clim_test\"] = (set_info[\"epiweek\"] >= 201841) & (set_info[\"epiweek\"] <= 201940)\n",
    "set_info[\"clim_test_input\"] = set_info[\"epiweek\"] <= 201825\n",
    "set_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82330883-50bf-49e5-81a9-1b547eff53a1",
   "metadata": {},
   "source": [
    "##  2. Creating TimeSeries from Climate Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c006988-e93f-4267-aac0-7354629475bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_ts(data_df, val_cols, set_info = None, set_column = None, time_col = \"WeekStart\", time_format = \"%Y-%m-%d\"):\n",
    "    \"\"\"\n",
    "    Function creates a TimeSeries object from a DataFrame using only rows where set_column is true\n",
    "\n",
    "    Parameters:\n",
    "    data_df (DataFrame): DataFrame containing TimeSeries data (assumed in chronological order)\n",
    "    val_cols (list or string): Name(s) of the column(s) in data_df containing the values to make a TimeSeries (will be multivariate if a list with more than 1 element)\n",
    "    set_info (DataFrame): DataFrame to merge into data_df (assumed with Year, Week, and epiweek columns) containg set information\n",
    "    set_column (string): Name of the column containing booleans to be used for inclusion/exclusion in the output TimeSeries\n",
    "    time_col (string): Name of the column to be used as the time-axis of the output series\n",
    "    time_format (string): Format of the time column\n",
    "\n",
    "    Returns:\n",
    "    curr_ts: TimeSeries filtered based on set_column with values based on val_cols\n",
    "    \"\"\"\n",
    "    data_df[time_col] = pd.to_datetime(data_df[time_col], format = time_format)\n",
    "    data_df = data_df.set_index(time_col)\n",
    "\n",
    "    if set_info is not None:\n",
    "        #We assume the set_info DataFrame has Year, Week, and epiweek columns we can join on \n",
    "        data_df = pd.merge(data_df, set_info, how = \"left\", on = [\"Year\", \"Week\", \"epiweek\"]) \n",
    "    if set_column is not None:\n",
    "        data_df = data_df[data_df[set_column]]\n",
    "    \n",
    "    curr_ts = TimeSeries.from_times_and_values(times = data_df.index, values = data_df[val_cols], columns = val_cols)\n",
    "    return curr_ts\n",
    "\n",
    "\n",
    "def create_ts_list(data_df, val_cols, group_col, group_vals, set_info = None, set_column = None, time_col = \"WeekStart\", time_format = \"%Y-%m-%d\"):\n",
    "    \"\"\"\n",
    "    Function creates a list TimeSeries objects from a DataFrame. Each list element is a TimeSeries for a particular unit from group_vals, based on the group_col\n",
    "\n",
    "    Parameters:\n",
    "    data_df (DataFrame): DataFrame containing TimeSeries data (assumed in chronological order)\n",
    "    val_cols (list or string): Name(s) of the column(s) in data_df containing the values to make a TimeSeries (will be multivariate if a list with more than 1 element)\n",
    "    group_col (string): Name of the column to match with the units from group_vals\n",
    "    group_vals (list): List of values in the group_col to make a TimeSeries for. The list returned by this function will have a TimeSeries for each element in group_vals.\n",
    "    set_info (DataFrame): DataFrame to merge into data_df (assumed with Year, Week, and epiweek columns) containg set information\n",
    "    set_column (string): Name of the column containing booleans to be used for inclusion/exclusion in the output TimeSeries\n",
    "    time_col (string): Name of the column to be used as the time-axis of the output series\n",
    "    time_format (string): Format of the time column\n",
    "\n",
    "    Returns:\n",
    "    to_ret: list of TimeSeries, one for each element in group_vals\n",
    "    \"\"\"\n",
    "    to_ret = []\n",
    "    if set_info is not None:\n",
    "        data_df = pd.merge(data_df, set_info, how = \"left\", on = [\"Year\", \"Week\", \"epiweek\"])\n",
    "        \n",
    "    for curr_unit in group_vals:\n",
    "        curr_unit_df = data_df[data_df[group_col] == curr_unit]\n",
    "        curr_unit_ts = create_ts(curr_unit_df, val_cols, set_column = set_column, time_col = time_col, time_format = time_format) #No need to pass set_info here since we have already merged in this function\n",
    "        to_ret.append(curr_unit_ts)\n",
    "    return to_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6504a31c-68d1-4d7c-bfcc-8b7053907174",
   "metadata": {},
   "outputs": [],
   "source": [
    "uf_list = climate_df[\"uf\"].unique()\n",
    "uf_mapper = climate_df[[\"uf\", \"uf_name\"]].copy().drop_duplicates()\n",
    "uf_dict = dict(zip(uf_mapper[\"uf\"], uf_mapper[\"uf_name\"]))\n",
    "uf_name_list = [uf_dict[curr_uf] for curr_uf in uf_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcad9447-13fc-452b-aa72-2d69faaf2fe7",
   "metadata": {},
   "source": [
    "### 2.1 Pre-transforms\n",
    "Before we create TimeSeries for forecasting, we have to transform some values that have restricted forecast ranges. These are mainly precipitation values (which cannot be below 0) and rainy days (which are restricted from 0 to 7). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5920336b-6b1e-493c-8481-614e30917d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_trans_cols = [\"precip_min\", \"precip_med\", \"precip_max\"] #Logp1 transform the precipitation values\n",
    "#logit_trans_cols = [\"rainy_days\"] #Limit rainy days between 0-7 using our custom logit function\n",
    "proc_climate_df = climate_df.copy()\n",
    "\n",
    "#Compute pre-processed version of the climate_df, log-transforming the precipitation variables, which have to always be positive\n",
    "for curr_col in log_trans_cols:\n",
    "    proc_climate_df[curr_col] = np.log1p(climate_df[curr_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654eccb4-9925-4f04-8f9f-b33c1877a631",
   "metadata": {},
   "source": [
    "### 2.2 Create TimeSeries\n",
    "Create the TimeSeries, we have 6 primary ones here, climate train1, val1, train2, val2, train3, and val3 (referred to as Clim 1a,b,c and ClimVal 1a,b,c in documentation). Once we have the 6 series, we then prepare them for modelling by scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ad8de1-0624-4532-98ea-4541aadbef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ts_list(plot_list, label_list, plot_comps = None, dims = (7,4), width = 18, height = 24, low_quantile = 0.025, high_quantile = 0.975, fig = None, axs = None, prefixes = None):\n",
    "    fig, axs = plt.subplots(dims[0], dims[1], figsize = (width,  height))\n",
    "\n",
    "    if prefixes is None:\n",
    "        prefixes = [\"\" for temp in plot_list]\n",
    "\n",
    "    for ts_list, curr_prefix in zip(plot_list, prefixes):\n",
    "        for curr_ts, curr_label, curr_ax in zip(ts_list, label_list, axs.flatten()):\n",
    "            if plot_comps is not None:\n",
    "                to_plot = curr_ts[plot_comps]\n",
    "            else:\n",
    "                to_plot = curr_ts\n",
    "        \n",
    "            to_plot.plot(ax = curr_ax, label = curr_prefix, low_quantile = low_quantile, high_quantile = high_quantilte)\n",
    "            curr_ax.set_title(curr_label)\n",
    "            curr_ax.set_xlabel(\"\")\n",
    "\n",
    "#Revert pre-processing\n",
    "def invert_pre_process_ts(curr_ts, clim_var_names = clim_var_names, log_trans_cols = log_trans_cols, logit_trans_cols = []):\n",
    "    curr_builder = []\n",
    "    for curr_var in clim_var_names:\n",
    "        temp = curr_ts[curr_var] \n",
    "        if curr_var in log_trans_cols:\n",
    "            temp = temp.map(np.expm1)\n",
    "        elif curr_var in logit_trans_cols:\n",
    "            temp = temp.map(lambda y: inv_log_bounds(y, 0, 7))\n",
    "        curr_builder.append(temp)\n",
    "    \n",
    "    result_ts = concatenate(curr_builder, axis = 1)\n",
    "    return result_ts\n",
    "\n",
    "def invert_pre_process_ts_list(curr_ts_list, clim_var_names = clim_var_names, log_trans_cols = log_trans_cols, logit_trans_cols = []):\n",
    "    to_ret = []\n",
    "    for curr_ts in curr_ts_list:\n",
    "        result = invert_pre_process_ts(curr_ts, clim_var_names = clim_var_names, log_trans_cols = log_trans_cols, logit_trans_cols = logit_trans_cols)\n",
    "        to_ret.append(result)\n",
    "    return to_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f58be-6da9-455b-a891-e47ab0def87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_col = \"uf\"\n",
    "\n",
    "#Training set without the pre-processing, mainly for normalising error metrics\n",
    "train1 = create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_train1\")\n",
    "train2 = create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_train2\")\n",
    "train3 = create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_train3\")\n",
    "\n",
    "#Training sets with pre-processing for actual model training\n",
    "train1_log = create_ts_list(proc_climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_train1\")\n",
    "train2_log = create_ts_list(proc_climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_train2\")\n",
    "train3_log = create_ts_list(proc_climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_train3\")\n",
    "\n",
    "#Note that since the validation and test sets are only used for evaluation / error calculation, we use the non pre-processed version (no log-transforms) \n",
    "val1 = create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_val1\")\n",
    "val2 = create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_val2\")\n",
    "val3 = create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_val3\")\n",
    "\n",
    "test_input_log = create_ts_list(proc_climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_test_input\") # Input sequence for the final model to evaluate on test set\n",
    "test = create_ts_list(climate_df, clim_var_names, group_col, uf_list, set_info = set_info, set_column = \"clim_test\") #Test set for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546c55c-5171-436b-bb19-dc0e92ed90ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling values\n",
    "clim_scaler1 = Scaler(global_fit = global_scale)\n",
    "clim_scaler1.fit(train1_log)\n",
    "\n",
    "#Only scale training sets \n",
    "train1_log_s = clim_scaler1.transform(train1_log)\n",
    "train2_log_s = clim_scaler1.transform(train2_log)\n",
    "train3_log_s = clim_scaler1.transform(train3_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3ee226-9d48-451f-a505-a25236e707e5",
   "metadata": {},
   "source": [
    "## 3. Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dffff32-3d95-4230-9498-d3e84a52a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_study(curr_study, add_path):\n",
    "    curr_write_dir = os.path.join(tuning_dir, add_path)\n",
    "    os.makedirs(curr_write_dir, exist_ok = True)\n",
    "    curr_trial_df = curr_study.trials_dataframe()\n",
    "    curr_best_params = curr_study.best_params\n",
    "\n",
    "    dict_as_json(curr_best_params, os.path.join(curr_write_dir, \"best_params.json\"))\n",
    "    curr_trial_df.to_csv(os.path.join(curr_write_dir, \"trial_df.csv\"), index = False)\n",
    "    save_pickle(curr_study, os.path.join(curr_write_dir, \"study_obj.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b62532c-800d-4054-bdd3-9e23914587ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_length = 15\n",
    "#We use 53 as the base forecast length because some years have 53 weeks\n",
    "if approach == \"forecast_gap\": #\"Forecast the gap\" approach\n",
    "    output_chunk_length = 1 #We assume 1-step ahead for the forecast the gap approach\n",
    "    output_chunk_shift = 0\n",
    "    forecast_length = 53 + gap_length\n",
    "else: #\"One shot forecast\" approach\n",
    "    output_chunk_length = 53 #If we are doing a 1-shot forecast, we set output_chunk_length to 53 since some years have 53 weeks.\n",
    "    output_chunk_shift = gap_length #We set an output_chunk_shift equal to the gap size\n",
    "    forecast_length = output_chunk_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ff2ace-17ab-4b10-a06e-d31f7def31b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def climate_objective(trial, model_mode, retrain_per_fold = retrain_per_fold):\n",
    "    #Tune the lags hyperparameter\n",
    "    lags = trial.suggest_int(\"lags\", 4, 208)\n",
    "    encoders = {\"datetime_attribute\": {\"future\": [\"month\", \"year\"]}, \n",
    "                    \"transformer\": Scaler()}\n",
    "    #Create the model, either a LinearRegression, RandomForest, or XGBoost\n",
    "    if model_mode == \"LinearRegression\":\n",
    "        curr_model1 = LinearRegressionModel(lags = lags, lags_future_covariates = (lags, output_chunk_length), add_encoders = encoders, output_chunk_length = output_chunk_length, output_chunk_shift = output_chunk_shift)\n",
    "        curr_model2 = LinearRegressionModel(lags = lags, lags_future_covariates = (lags, output_chunk_length), add_encoders = encoders, output_chunk_length = output_chunk_length, output_chunk_shift = output_chunk_shift)\n",
    "        curr_model3 = LinearRegressionModel(lags = lags, lags_future_covariates = (lags, output_chunk_length), add_encoders = encoders, output_chunk_length = output_chunk_length, output_chunk_shift = output_chunk_shift)\n",
    "    elif model_mode == \"RandomForest\":\n",
    "        curr_model1 = RandomForestModel(lags = lags, lags_future_covariates = (lags, output_chunk_length), add_encoders = encoders, output_chunk_length = output_chunk_length, output_chunk_shift = output_chunk_shift, random_state = random_seed)\n",
    "        curr_model2 = RandomForestModel(lags = lags, lags_future_covariates = (lags, output_chunk_length), add_encoders = encoders, output_chunk_length = output_chunk_length, output_chunk_shift = output_chunk_shift, random_state = random_seed)\n",
    "        curr_model3 = RandomForestModel(lags = lags, lags_future_covariates = (lags, output_chunk_length), add_encoders = encoders, output_chunk_length = output_chunk_length, output_chunk_shift = output_chunk_shift, random_state = random_seed)\n",
    "    else:\n",
    "        curr_model1 = XGBModel(lags = lags, lags_future_covariates = (lags, output_chunk_length), add_encoders = encoders, output_chunk_length = output_chunk_length, output_chunk_shift = output_chunk_shift, random_state = random_seed)\n",
    "        curr_model2 = XGBModel(lags = lags, lags_future_covariates = (lags, output_chunk_length), add_encoders = encoders, output_chunk_length = output_chunk_length, output_chunk_shift = output_chunk_shift, random_state = random_seed)\n",
    "        curr_model3 = XGBModel(lags = lags, lags_future_covariates = (lags, output_chunk_length), add_encoders = encoders, output_chunk_length = output_chunk_length, output_chunk_shift = output_chunk_shift, random_state = random_seed)\n",
    "    \n",
    "    #Fit the model using the training set\n",
    "    curr_model1.fit(series = train1_log_s)\n",
    "\n",
    "    if retrain_per_fold: #If we retrain per fold, we have to fit curr_model2 and 3, then predict with all three models\n",
    "        curr_model2.fit(series = train2_log_s)\n",
    "        curr_model3.fit(series = train3_log_s)\n",
    "        \n",
    "        preds1_log_s = curr_model1.predict(series = train1_log_s, n = forecast_length)\n",
    "        preds2_log_s = curr_model2.predict(series = train2_log_s, n = forecast_length)\n",
    "        preds3_log_s = curr_model3.predict(series = train3_log_s, n = forecast_length)\n",
    "    else: #If we do not retrain per fold, all predictions done only with curr_model1\n",
    "        preds1_log_s = curr_model1.predict(series = train1_log_s, n = forecast_length)\n",
    "        preds2_log_s = curr_model1.predict(series = train2_log_s, n = forecast_length)\n",
    "        preds3_log_s = curr_model1.predict(series = train3_log_s, n = forecast_length)\n",
    "\n",
    "    #Invert the scaling\n",
    "    preds1_log = clim_scaler1.inverse_transform(preds1_log_s) \n",
    "    preds2_log = clim_scaler1.inverse_transform(preds2_log_s)\n",
    "    preds3_log = clim_scaler1.inverse_transform(preds3_log_s)\n",
    "\n",
    "    #Invert the log transforms\n",
    "    preds1 = invert_pre_process_ts_list(preds1_log)\n",
    "    preds2 = invert_pre_process_ts_list(preds2_log)\n",
    "    preds3 = invert_pre_process_ts_list(preds3_log)\n",
    "    \n",
    "    #Compute the MSE \n",
    "    loss1 = norm_metric_list(val1, preds1, train1, comp_summ_func = np.mean, series_summ_func = np.mean, metric = mae)\n",
    "    loss2 = norm_metric_list(val2, preds2, train1, comp_summ_func = np.mean, series_summ_func = np.mean, metric = mae)\n",
    "    loss3 = norm_metric_list(val3, preds3, train1, comp_summ_func = np.mean, series_summ_func = np.mean, metric = mae)\n",
    "    losses = np.array([loss1, loss2, loss3])\n",
    "    val_loss = np.mean(losses)\n",
    "    return(val_loss)\n",
    "    \n",
    "def print_callback(study, trial):\n",
    "    print(f\"Current Trial: {trial._trial_id}\")\n",
    "    print(f\"Current value: {trial.value}, Current params: {trial.params}\")\n",
    "    print(f\"Best value: {study.best_value}, Best params: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7065acdd-335f-445d-81a2-0cb43f51418b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampler1 = TPESampler(seed = random_seed)\n",
    "study1 = optuna.create_study(direction = \"minimize\", sampler = sampler1)\n",
    "\n",
    "study1.optimize(lambda trial: climate_objective(trial, \"LinearRegression\", retrain_per_fold = retrain_per_fold), callbacks = [print_callback], n_trials = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d21f323-2ad6-4809-833b-1d7432960947",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_study(study1, \"LinearRegression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eea6b2-d019-47c2-9ad2-881f8eb45a00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampler2 = TPESampler(seed = random_seed)\n",
    "study2 = optuna.create_study(direction = \"minimize\", sampler = sampler2)\n",
    "\n",
    "study2.optimize(lambda trial: climate_objective(trial, \"RandomForest\", retrain_per_fold = retrain_per_fold), callbacks = [print_callback], n_trials = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13795455-6bf7-4977-bdb3-2f36ce9b41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_study(study2, \"RandomForest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb63f6b-4ebc-47fd-a20a-94ceeb12bc31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampler3 = TPESampler(seed = random_seed)\n",
    "study3 = optuna.create_study(direction = \"minimize\", sampler = sampler3)\n",
    "\n",
    "study3.optimize(lambda trial: climate_objective(trial, \"XGBoost\", retrain_per_fold = retrain_per_fold), callbacks = [print_callback], n_trials = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1208f335-edce-4a2f-a11f-63faa2b11af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_study(study3, \"XGBoost\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
